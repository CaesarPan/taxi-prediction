{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickle = '/home/lashi/assets/clean-data/merged_141516.pickle'\n",
    "df = pd.read_pickle(df_pickle).reset_index(drop=True)\n",
    "train_set, test_set = train_test_split(df, test_size=0.05)\n",
    "train_set = train_set.reset_index(drop=True)\n",
    "test_set = test_set.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2744654\n",
      "144456\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   LocationID        Borough                     Zone ClusterSize  \\\n",
      "0           1            EWR           Newark Airport        None   \n",
      "1           2         Queens              Jamaica Bay        None   \n",
      "2           3          Bronx  Allerton/Pelham Gardens        None   \n",
      "3           4      Manhattan            Alphabet City        None   \n",
      "4           5  Staten Island            Arden Heights        None   \n",
      "\n",
      "  ClusterGlobalIDs KmeansMSE Centroids  \n",
      "0             None      None      None  \n",
      "1             None      None      None  \n",
      "2             None      None      None  \n",
      "3             None      None      None  \n",
      "4             None      None      None  \n"
     ]
    }
   ],
   "source": [
    "pickup_table = '/home/lashi/assets/taxi_zones/taxi_zone_lookup.csv'\n",
    "taxi_zone_df = pd.read_csv(pickup_table)\n",
    "kmeans_df = taxi_zone_df.drop(['service_zone'], axis=1)\n",
    "kmeans_df['ClusterSize'] = None\n",
    "kmeans_df['ClusterGlobalIDs'] = None\n",
    "kmeans_df['KmeansMSE'] = None\n",
    "kmeans_df['Centroids'] = None\n",
    "\n",
    "kmeans_df.astype('object')\n",
    "\n",
    "print(kmeans_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished clustering up to id: 0\n",
      "Finished clustering up to id: 10\n",
      "Finished clustering up to id: 20\n",
      "Finished clustering up to id: 30\n",
      "Finished clustering up to id: 40\n",
      "Finished clustering up to id: 50\n",
      "Finished clustering up to id: 60\n",
      "Finished clustering up to id: 70\n",
      "Finished clustering up to id: 80\n",
      "Finished clustering up to id: 90\n",
      "Finished clustering up to id: 100\n",
      "Finished clustering up to id: 110\n",
      "Finished clustering up to id: 120\n",
      "Finished clustering up to id: 130\n",
      "Finished clustering up to id: 140\n",
      "Finished clustering up to id: 150\n",
      "Finished clustering up to id: 160\n",
      "Finished clustering up to id: 170\n",
      "Finished clustering up to id: 180\n",
      "Finished clustering up to id: 190\n",
      "Finished clustering up to id: 200\n",
      "Finished clustering up to id: 210\n",
      "Finished clustering up to id: 220\n",
      "Finished clustering up to id: 230\n",
      "Finished clustering up to id: 240\n",
      "Finished clustering up to id: 250\n",
      "Finished clustering up to id: 260\n",
      "Finished clustering up to id: 270\n"
     ]
    }
   ],
   "source": [
    "# Run K-means on training set to get clusters\n",
    "clusters_dict = dict()\n",
    "global_cluster_id = 0\n",
    "num_clusters = 10\n",
    "cluster_id = np.zeros((len(train_set),))\n",
    "\n",
    "for large_id in range(280):\n",
    "    if large_id % 10 == 0:\n",
    "        print(\"Finished clustering up to id:\", large_id)\n",
    "    df_of_id = train_set.loc[train_set['pickup_id'] == large_id]\n",
    "    if not df_of_id.empty:\n",
    "        PU_merc_of_id = np.stack(df_of_id['pickup_merc_x_y'].values, axis=0)\n",
    "        if len(PU_merc_of_id) >= num_clusters:\n",
    "            kmeans = KMeans(n_clusters=num_clusters, n_jobs=-1).fit(PU_merc_of_id)\n",
    "            cluster_centroids = kmeans.cluster_centers_\n",
    "            labels = kmeans.labels_\n",
    "            \n",
    "            # inserting the cluster size into the kmeans dataframe\n",
    "            kmeans_df.loc[kmeans_df.LocationID == large_id, 'ClusterSize'] = num_clusters\n",
    "            \n",
    "            # inserting the cluster error into the kmeans dataframe\n",
    "            kmeans_df.loc[kmeans_df.LocationID == large_id, 'KmeansMSE'] = kmeans.inertia_\n",
    "        else:\n",
    "            cluster_centroids = PU_merc_of_id\n",
    "            labels = np.asarray(range(len(cluster_centroids)))\n",
    "            \n",
    "            # inserting the cluster size into the kmeans dataframe\n",
    "            kmeans_df.loc[kmeans_df.LocationID == large_id, 'ClusterSize'] = len(cluster_centroids)\n",
    "            kmeans_df.loc[kmeans_df.LocationID == large_id, 'KmeansMSE'] = 0.0\n",
    "\n",
    "        clusters_in_large_id = []\n",
    "        cluster_centroids_in_large_id = []\n",
    "        for k, cluster in enumerate(cluster_centroids):\n",
    "            cluster_centroids_in_large_id.append(cluster)\n",
    "            clusters_dict[global_cluster_id] = cluster\n",
    "            k_group = df_of_id.loc[labels==k]\n",
    "            k_group_i = k_group.index.values\n",
    "            cluster_id[k_group_i] = global_cluster_id\n",
    "            clusters_in_large_id.append(global_cluster_id)\n",
    "            global_cluster_id += 1\n",
    "\n",
    "            # inserting the cluster dictionary keys into the kmeans dataframe\n",
    "#         print(clusters_dict[large_id])\n",
    "        kmeans_df.at[large_id-1, 'ClusterGlobalIDs'] = [clusters_in_large_id]\n",
    "        kmeans_df.at[large_id-1, 'Centroids'] = [cluster_centroids_in_large_id]\n",
    "\n",
    "train_set['cluster_id'] = cluster_id.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cluster distributions for each Pickup ID using train_set K-means centroids\n",
    "df_cluster_ids = train_set.groupby(['pickup_id','cluster_id']).size().reset_index()\n",
    "df_pickup_ids = train_set.groupby(['pickup_id']).size().reset_index()\n",
    "cluster_dist_dict = dict()\n",
    "\n",
    "for i in range(len(df_pickup_ids)):\n",
    "    pickup_id = df_pickup_ids['pickup_id'][i]\n",
    "    id_set = df_cluster_ids.loc[df_cluster_ids['pickup_id']==pickup_id].reset_index()\n",
    "    num_clusters = len(id_set)\n",
    "    cluster_dist = np.zeros(num_clusters)\n",
    "    for cluster in range(num_clusters):\n",
    "        cluster_dist[cluster] = id_set[0].loc[cluster]\n",
    "    cluster_dist = cluster_dist/np.sum(cluster_dist)\n",
    "    cluster_dist_dict[pickup_id] = cluster_dist\n",
    "    \n",
    "# print(cluster_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n"
     ]
    }
   ],
   "source": [
    "# Distribute test_set data into different clusters based on distributions above\n",
    "loc_ids = test_set['pickup_id'].values\n",
    "cluster_ids = np.zeros(loc_ids.size)\n",
    "\n",
    "for i,loc_id in enumerate(loc_ids):\n",
    "    loc_id = loc_ids[i]\n",
    "    if i%10000 == 0:\n",
    "        print(i)\n",
    "    if kmeans_df.iloc[int(loc_id-1)]['ClusterGlobalIDs'] == None:\n",
    "        cluster_ids[i] = None\n",
    "    else:\n",
    "        cluster_ids[i] = np.random.choice(kmeans_df.iloc[int(loc_id-1)]['ClusterGlobalIDs'][0],p=cluster_dist_dict[loc_id])\n",
    "\n",
    "test_set['cluster_id'] = cluster_ids.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of classified test_set data (predicted distribution)\n",
    "df_cluster_ids = test_set.groupby(['pickup_id','cluster_id']).size().reset_index()\n",
    "df_pickup_ids = test_set.groupby(['pickup_id']).size().reset_index()\n",
    "cluster_dist_test_dict = dict()\n",
    "\n",
    "for i in range(len(df_pickup_ids)):\n",
    "    pickup_id = df_pickup_ids['pickup_id'][i]\n",
    "    if kmeans_df.iloc[int(pickup_id-1)]['ClusterGlobalIDs'] == None:\n",
    "        cluster_dist_test_dict[pickup_id] = np.zeros(1)\n",
    "    else:\n",
    "        global_ids = kmeans_df.iloc[int(pickup_id-1)]['ClusterGlobalIDs'][0]\n",
    "        num_clusters = len(global_ids)\n",
    "        cluster_dist = np.zeros(num_clusters)\n",
    "        id_set = df_cluster_ids.loc[df_cluster_ids['pickup_id']==pickup_id].reset_index()\n",
    "        num_present = len(id_set)\n",
    "        for cluster in range(num_present):\n",
    "            curr_id = id_set['cluster_id'].loc[cluster]\n",
    "            ind = global_ids.index(curr_id)\n",
    "            cluster_dist[ind] = id_set[0].loc[cluster]\n",
    "        cluster_dist = cluster_dist/np.sum(cluster_dist)\n",
    "        cluster_dist_test_dict[pickup_id] = cluster_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 50.0\n",
      "5000 79.0\n",
      "10000 48.0\n",
      "15000 142.0\n",
      "20000 129.0\n",
      "25000 79.0\n",
      "30000 138.0\n",
      "35000 239.0\n",
      "40000 263.0\n",
      "45000 161.0\n",
      "50000 246.0\n",
      "55000 79.0\n",
      "60000 48.0\n",
      "65000 239.0\n",
      "70000 107.0\n",
      "75000 90.0\n",
      "None: 78364 44.0\n",
      "80000 229.0\n",
      "85000 234.0\n",
      "90000 234.0\n",
      "95000 114.0\n",
      "100000 262.0\n",
      "105000 79.0\n",
      "110000 143.0\n",
      "115000 97.0\n",
      "120000 143.0\n",
      "125000 229.0\n",
      "130000 68.0\n",
      "135000 75.0\n",
      "140000 234.0\n"
     ]
    }
   ],
   "source": [
    "# Get truth cluster_ids from test_set data (based on K-means centroids)\n",
    "true_cluster_id = np.zeros(len(test_set))\n",
    "for i in range(len(test_set)):\n",
    "    coord = test_set.iloc[i].pickup_merc_x_y\n",
    "    pu_id = test_set.iloc[i].pickup_id\n",
    "    if i%5000 == 0:\n",
    "        print(i,pu_id)\n",
    "    if kmeans_df.iloc[int(pu_id-1)].Centroids == None:\n",
    "        print('None:',i,pu_id)\n",
    "        selected_id = 1000000\n",
    "    else:\n",
    "        centroids = kmeans_df.iloc[int(pu_id-1)].Centroids[0]\n",
    "        min_error = 10000000\n",
    "        for ind, centroid in enumerate(centroids):\n",
    "            error = np.linalg.norm(abs(coord-centroid))\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                selected_id = kmeans_df.iloc[int(pu_id-1)].ClusterGlobalIDs[0][ind]\n",
    "\n",
    "        true_cluster_id[i] = selected_id\n",
    "\n",
    "test_set['true_cluster_id'] = true_cluster_id\n",
    "    \n",
    "\n",
    "# print(kmeans_df.iloc[50-1].ClusterGlobalIDs)\n",
    "# print(kmeans_df.iloc[50-1].Centroids[0][0])\n",
    "# print(abs(kmeans_df.iloc[50-1].Centroids[0][0] - test_set.iloc[0].pickup_merc_x_y))\n",
    "# print(np.linalg.norm(abs(kmeans_df.iloc[50-1].Centroids[0][0] - test_set.iloc[0].pickup_merc_x_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get distribution of true test_set data (true distribution)\n",
    "df_cluster_ids = test_set.groupby(['pickup_id','true_cluster_id']).size().reset_index()\n",
    "df_pickup_ids = test_set.groupby(['pickup_id']).size().reset_index()\n",
    "cluster_dist_true_dict = dict()\n",
    "\n",
    "for i in range(len(df_pickup_ids)):\n",
    "    pickup_id = df_pickup_ids['pickup_id'][i]\n",
    "    if kmeans_df.iloc[int(pickup_id-1)]['ClusterGlobalIDs'] == None:\n",
    "        cluster_dist_test_dict[pickup_id] = np.zeros(1)\n",
    "    else:\n",
    "        global_ids = kmeans_df.iloc[int(pickup_id-1)]['ClusterGlobalIDs'][0]\n",
    "        num_clusters = len(global_ids)\n",
    "        cluster_dist = np.zeros(num_clusters)\n",
    "        id_set = df_cluster_ids.loc[df_cluster_ids['pickup_id']==pickup_id].reset_index()\n",
    "        num_present = len(id_set)\n",
    "        for cluster in range(num_present):\n",
    "            curr_id = id_set['true_cluster_id'].loc[cluster]\n",
    "            ind = global_ids.index(curr_id)\n",
    "            cluster_dist[ind] = id_set[0].loc[cluster]\n",
    "        cluster_dist = cluster_dist/np.sum(cluster_dist)\n",
    "        cluster_dist_true_dict[pickup_id] = cluster_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16852\n",
      "144456\n",
      "0.1166583596389212\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for i in range(len(test_set)):\n",
    "    if test_set.cluster_id[i] == test_set.true_cluster_id[i]:\n",
    "        num_correct += 1\n",
    "\n",
    "print(num_correct)\n",
    "print(len(test_set))\n",
    "print(num_correct/len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
